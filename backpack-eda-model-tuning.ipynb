# %% [code] {"execution":{"iopub.status.busy":"2025-02-01T13:48:31.424937Z","iopub.execute_input":"2025-02-01T13:48:31.425267Z","iopub.status.idle":"2025-02-01T13:48:32.905420Z","shell.execute_reply.started":"2025-02-01T13:48:31.425230Z","shell.execute_reply":"2025-02-01T13:48:32.904318Z"}}
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import missingno as msno
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# %% [markdown]
# # 1ï¸âƒ£ Introduction
# 

# %% [markdown]
# ## ğŸ† Overview of the competition
# 
# In this playground competition we should predict backpack price based on different features, such as weight capacity, brand, material, size, compartments,  laptop compartment, waterproof, style and color.
#  

# %% [markdown]
# ## ğŸ¯ Problem statement & objectives
# 
# Backpacks are an essential part of everyday life for many people, especially in modern times when individuals carry a wide variety of personal and professional items daily. Whether itâ€™s for work, school, travel, or outdoor activities, backpacks provide the convenience of carrying everything needed for the day. One group that particularly benefits from this versatility is programmers, who often need to carry a range of equipment, from laptops and chargers to notebooks and headphones, all essential to their daily tasks.
# 
# This is where our challenge lies: using machine learning to predict key features of backpacks based on their characteristics. The goal is not just to create a predictive model, but to better understand the dataset, create meaningful insights, and ultimately fine-tune our model to predict with greater accuracy ğŸ“ˆ.
# 
# Our objectives for this notebook are:
# * ğŸ“Š Exploratory Data Analysis (EDA): Conduct a thorough analysis of the dataset to uncover insights about how different features correlate with each other and the target variable ğŸ”
# * ğŸš€ Baseline Model: Establish a simple baseline model to set performance expectations and identify room for improvement ğŸ§‘â€ğŸ’»
# * ğŸ› ï¸Feature Engineering: Identify and create new features that may enhance model performance and better represent the data ğŸ’¡
# * ğŸ›ï¸ Hyperparameter Tuning: Optimize model performance by adjusting hyperparameters using Optuna to find the most effective configuration âš™ï¸
# * ğŸ“ˆ Model Evaluation: Assess the performance of the model using various metrics and discuss potential improvements moving forward ğŸ¯
# Through this journey, weâ€™ll focus on not only building a model but also enhancing our understanding of the data, ensuring that we leverage the best techniques to achieve high predictive accuracy. ğŸ’ª

# %% [markdown]
# # 2ï¸âƒ£ Exploratory Data Analysis (EDA) 
# 
# 

# %% [markdown]
# ## ğŸ“Š Data overview (shape, missing values, types)
# 

# %% [markdown]
# Let's take a look on training data.

# %% [code] {"execution":{"iopub.status.busy":"2025-02-01T13:48:32.906830Z","iopub.execute_input":"2025-02-01T13:48:32.907495Z","iopub.status.idle":"2025-02-01T13:48:34.238429Z","shell.execute_reply.started":"2025-02-01T13:48:32.907450Z","shell.execute_reply":"2025-02-01T13:48:34.237283Z"}}
X_train = pd.read_csv('/kaggle/input/playground-series-s5e2/train.csv')
X_test = pd.read_csv('/kaggle/input/playground-series-s5e2/test.csv')
sample_subm = pd.read_csv('/kaggle/input/playground-series-s5e2/sample_submission.csv')

# %% [markdown]
# From first look we're having a small number of columns with categorical and numerical features.

# %% [code] {"execution":{"iopub.status.busy":"2025-02-01T13:48:34.240385Z","iopub.execute_input":"2025-02-01T13:48:34.240797Z","iopub.status.idle":"2025-02-01T13:48:34.408297Z","shell.execute_reply.started":"2025-02-01T13:48:34.240736Z","shell.execute_reply":"2025-02-01T13:48:34.406891Z"}}
X_train.info()

# %% [markdown]
# Good, we're have 300k rows and 11 columns in dataset. What about missing values? 

# %% [code] {"execution":{"iopub.status.busy":"2025-02-01T13:48:34.409691Z","iopub.execute_input":"2025-02-01T13:48:34.410017Z","iopub.status.idle":"2025-02-01T13:48:34.526834Z","shell.execute_reply.started":"2025-02-01T13:48:34.409994Z","shell.execute_reply":"2025-02-01T13:48:34.525937Z"}}
X_train.isna().sum()

# %% [markdown]
# We're having missing values in all columns, except for id, compartments and price. The biggest number of missing values in color and brand column. Is the same thing in test data?

# %% [code] {"execution":{"iopub.status.busy":"2025-02-01T13:48:34.527874Z","iopub.execute_input":"2025-02-01T13:48:34.528140Z","iopub.status.idle":"2025-02-01T13:48:34.609349Z","shell.execute_reply.started":"2025-02-01T13:48:34.528117Z","shell.execute_reply":"2025-02-01T13:48:34.608438Z"}}
X_test.isna().sum()

# %% [markdown]
# Yep, the same thing goes here. Now let's take a look on descriptive statistics.

# %% [code] {"execution":{"iopub.status.busy":"2025-02-01T13:48:34.610424Z","iopub.execute_input":"2025-02-01T13:48:34.610713Z","iopub.status.idle":"2025-02-01T13:48:34.953534Z","shell.execute_reply.started":"2025-02-01T13:48:34.610676Z","shell.execute_reply":"2025-02-01T13:48:34.952203Z"}}
X_train.describe(include='all')

# %% [markdown]
# ## ğŸ“ˆ Visualizing class distributions
# 

# %% [markdown]
# Now let's vizualize our class distributions. Starting from numerical features.

# %% [code] {"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-02-01T13:48:34.954304Z","iopub.execute_input":"2025-02-01T13:48:34.954582Z","iopub.status.idle":"2025-02-01T13:48:43.194554Z","shell.execute_reply.started":"2025-02-01T13:48:34.954559Z","shell.execute_reply":"2025-02-01T13:48:43.192783Z"}}
fig, axs = plt.subplots(4,3, figsize=(10,10))
numerical_features = X_train.select_dtypes(exclude='object').columns
r = 0
for feat in numerical_features:
    c = 0
    while c<3:
        if c == 0:
            sns.kdeplot(ax=axs[r, c], data=X_train, x=feat)
            axs[r,c].set_title(f'Density Plot of {feat}')
            axs[r,c].set(xlabel=None)
        elif c == 1:
            sns.violinplot(ax=axs[r,c], data=X_train, x=feat)
            axs[r,c].set_title(f'Violin Plot of {feat}')
            axs[r,c].set(xlabel=None)
        elif c == 2:
            sns.boxplot(ax=axs[r,c], data=X_train, x=feat)
            axs[r,c].set_title(f'Box Plot of {feat}')
            axs[r,c].set(xlabel=None)
        c += 1
    r += 1

# %% [markdown]
# And now categorical features.

# %% [code] {"execution":{"iopub.status.busy":"2025-02-01T13:48:43.197801Z","iopub.execute_input":"2025-02-01T13:48:43.198183Z","iopub.status.idle":"2025-02-01T13:48:46.144311Z","shell.execute_reply.started":"2025-02-01T13:48:43.198153Z","shell.execute_reply":"2025-02-01T13:48:46.143139Z"}}
fig, axs = plt.subplots(4,2, figsize=(20,30))
cat_features = X_train.select_dtypes(include='object').columns
r = 0
c = 0
for feat in cat_features:
    if c == 2:
        c = 0
        r += 1
    counts = sns.countplot(ax=axs[r,c], data=X_train, x=feat)
    for p in counts.patches:
        counts.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()),  ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')
    
    axs[r,c].set_title(f'{feat}')
    axs[r,c].set(xlabel=None)

  
    c += 1

# %% [markdown]
# Our features seems to be more or less equally distributed across categories.

# %% [markdown]
# ## ğŸ”¬ Feature correlations & relationships

# %% [markdown]
# In this section we'll look at correlation between our features and build graphs features VS price.

# %% [code] {"execution":{"iopub.status.busy":"2025-02-01T13:48:46.146081Z","iopub.execute_input":"2025-02-01T13:48:46.146493Z","iopub.status.idle":"2025-02-01T13:48:46.471508Z","shell.execute_reply.started":"2025-02-01T13:48:46.146434Z","shell.execute_reply":"2025-02-01T13:48:46.470290Z"}}
sns.heatmap(X_train[numerical_features].corr(), annot=True)

# %% [markdown]
# Our features doesn't have any correlation.

# %% [markdown]
# Now let's take a look on features vs target.

# %% [code] {"execution":{"iopub.status.busy":"2025-02-01T13:48:46.472700Z","iopub.execute_input":"2025-02-01T13:48:46.473121Z","iopub.status.idle":"2025-02-01T13:48:49.163893Z","shell.execute_reply.started":"2025-02-01T13:48:46.473092Z","shell.execute_reply":"2025-02-01T13:48:49.162843Z"}}
fig, axs = plt.subplots(4,2, figsize=(20,30))
cat_features = X_train.select_dtypes(include='object').columns
r = 0
c = 0
for feat in cat_features:
    if c == 2:
        c = 0
        r += 1
    counts = sns.boxplot(ax=axs[r,c], data=X_train, x=feat, y='Price')
    
    axs[r,c].set_title(f'{feat}')
    axs[r,c].set(xlabel=None)

  
    c += 1

# %% [markdown]
# ## ğŸ’» Conclusions

# %% [markdown]
# From our EDA we can draw the next conclusions:
# * The most popular brand is Adidas;
# * The most popular material is Polyester;
# * The most popular size is Medium;
# * On average, backpacks have 5 compartments;
# * Laptop Compartment is the most crucial thing in a backpack;
# * Waterproof is also crucial, which is logical for keeping laptops and other important things safe (like money);
# * Messenger backpacks are the most popular option for style;
# * Preferred color is Pink;
# * Average weight capacity is 18 kg;
# * Average price is 81$.
# 
# After exploring distributions and relationships between our features, we can conclude the following:
# 
# * Features don't have any correlation;
# * Categorical features are equally distributed among categories;
# * Price is also equally distributed among different categories.
#   
# These insights suggest that our dataset maintains a balanced spread, which is promising for further modeling and analysis.

# %% [markdown]
# # 3ï¸âƒ£ Feature Engineering ğŸ—ï¸
# 
# 

# %% [markdown]
# Stay tuned...

# %% [code]
